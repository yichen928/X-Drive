foreground_loss_weight: 0.0  # additional scale, used as 1+x
bbox_drop_ratio: 0
bbox_add_ratio: 0
drop_cond_ratio: 0.25
bbox_add_num: 3

num_train_epochs: 2000
train_batch_size: 24
max_train_steps: null  # if null, will be overwrite by runner

num_workers: 32
prefetch_factor: 4
display_per_epoch: 20
display_per_n_min: 10

max_grad_norm: 1.0
set_grads_to_none: true
enable_xformers_memory_efficient_attention: false
unet_in_fp16: true
enable_unet_checkpointing: false  # if unet is not trainable, this is useless
enable_controlnet_checkpointing: false
noise_offset: 0.0 
train_with_same_offset: true

use_8bit_adam: false
adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 1e-2
adam_epsilon: 1e-08

# 28130 samples for train
learning_rate: 1e-4
lr_scheduler: cosine
gradient_accumulation_steps: 1
lr_num_cycles: 1
lr_power: 1.0

# steps parameter
lr_warmup_steps: 3000
checkpointing_steps: 20000
validation_steps: 20000
save_model_per_epoch: null

# validation
validation_before_run: true
validation_index: [204, 912, 1828, 2253, 4467, 5543]
validation_times: 2
validation_batch_size: 1
validation_show_box: false
validation_seed_global: false

pipeline_param:
  guidance_scale: 5  # if > 1, enable classifier-free guidance
  num_inference_steps: 50
  eta: 0.0
  controlnet_conditioning_scale: 1.0
  guess_mode: false
  use_zero_map_as_unconditional: false
  bbox_max_length: null  # on view_shared=False, train max 159, val max 117
  
use_ema: true